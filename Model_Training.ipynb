{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74dcab9-555f-4868-99f3-8322da5e34b5",
   "metadata": {},
   "source": [
    "# WCC Implementation, GPT-Neo from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deb39f-7eb5-475f-9988-aa5c8897b9ab",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3197b-55ba-4ae1-80fb-e4e8d7e52a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets tqdm matplotlib transformers accelerate deepspeed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce01479-e6fd-4495-a41d-cf1708f5e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import Trainer, Adafactor, get_scheduler\n",
    "from transformers import TrainingArguments\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import GPTNeoConfig, GPTNeoForCausalLM\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f38e79-aa43-4c5d-aab6-845d9f9a2e7b",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26083e0-5209-4fd9-9eb8-2f5a8a6a805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_context = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5473f-3659-4de5-a3bb-731a5e62ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv(\"sentences.csv\")\n",
    "sentences['Sentence'] = sentences['Sentence'].astype(str)\n",
    "print(f\"Total sentences before filtering: {len(sentences)}\")\n",
    "sentences = sentences[sentences[\"Word Count\"] < 25]\n",
    "print(f\"Total sentences after filtering: {len(sentences)}\")\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6e60e-1da7-43a7-b845-45237c8bd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wcc_token(sentence, word_count):\n",
    "     return f\"<{word_count}>{sentence}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62239fca-bd77-45cd-a171-cfab3d8860a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "sentences['Sentence_with_WCC'] = sentences.progress_apply(lambda row: add_wcc_token(row['Sentence'], row['Word Count']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1f991-bbe4-4fbd-90b5-68cb3eec7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = sentences.groupby('Paragraph ID').agg({\n",
    "    'Sentence_with_WCC': ' '.join,\n",
    "    'Word Count': 'sum',\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17313367-6069-4cc3-a9fa-373694941155",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs.to_csv('paragraphs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09b244-ff16-43b3-b08c-5cdf2ec93c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = pd.read_csv('paragraphs.csv').sample(350_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3b1b3-5235-4c50-8d7d-bc4d7ef0cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e030e71-fab8-4e7b-9c40-631985ee23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['Sentence'] = sentences['Sentence'].astype(str)\n",
    "print(f\"Total sentences before filtering: {len(sentences)}\")\n",
    "sentences = sentences[sentences[\"Word Count\"] < 25]\n",
    "print(f\"Total sentences after filtering: {len(sentences)}\")\n",
    "sentences.head()\n",
    "max_len = sentences[\"Word Count\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beebff-1315-43fb-be6f-45402388d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total paragraphs before filtering: {len(paragraphs)}\")\n",
    "paragraphs = paragraphs[paragraphs[\"Word Count\"] < max_context]\n",
    "print(f\"Total paragraphs after filtering: {len(paragraphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1460e06-6281-47a9-b8d0-a26300d7f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wcc_token_to_examples(examples):\n",
    "    examples['Sentence'] = [add_wcc_token(sentence, wc) for sentence, wc in zip(examples['Sentence'], examples['Word Count'])]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba53f86-4058-491c-adb4-0b45066ae3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(paragraphs.drop(columns=[\"Paragraph ID\"]))\n",
    "# Filter to keep only the relevant column\n",
    "dataset = dataset.map(lambda examples: {\"text\": examples[\"Sentence_with_WCC\"]}, remove_columns=[\"Word Count\"])\n",
    "\n",
    "next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8345d0-5e30-4b18-b660-6cea02d0a64d",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425af30-c6f3-4e0c-bcc8-5b04ba1a984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "config = GPTNeoConfig(\n",
    "    vocab_size=50257,  # You can use a larger vocab size if necessary\n",
    "    max_position_embeddings=2048,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    hidden_size=768,\n",
    "    intermediate_size=3072,\n",
    "    activation_function=\"gelu\",\n",
    "    attention_types=[[[\"global\", \"local\"], 6]],  # Global and local attention for all layers\n",
    ")\n",
    "\n",
    "# Initialize model from scratch\n",
    "model = GPTNeoForCausalLM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a51fd1-1e6c-4088-8d8b-b86d07b1b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf098710-c59e-460b-bda9-85ee90b9e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import GPTNeoConfig, GPTNeoForCausalLM\n",
    "from datasets import Dataset\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "print(\"Initial tokenizer size:\", len(tokenizer))\n",
    "print(\"Max len: \", max_len)\n",
    "\n",
    "additional_tokens = [f\"<{i}>\" for i in range(max_len + 1)]\n",
    "tokenizer.add_tokens(additional_tokens)\n",
    "\n",
    "print(\"Final tokenizer size:\", len(tokenizer))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Define data collator for language modeling\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Language modeling task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71dd7b-ab81-4e0b-8e19-3221cd8cd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt-neo-125m-from-scratch\",\n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=4, \n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,  \n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,  \n",
    "    adam_epsilon=1e-08,\n",
    "    num_train_epochs=3,  \n",
    "    logging_steps=100,\n",
    "    save_steps=0,  \n",
    "    deepspeed=\"ds_config.json\",  \n",
    "    fp16=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b93c63-213a-49c8-abd5-d26068e88b12",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90beaf1-25ff-4e0b-99c3-6dbd094e62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,  \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51dbc9-a11c-4ba1-a0c7-cd63ee85c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./gpt-neo-125m-final-350k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579abf0-b3c0-4fb2-b497-734a3e65f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<5>\"\n",
    "\n",
    "generated_text = model.generate(\n",
    "    input_ids=tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_length=100, \n",
    "    temperature=0.75, \n",
    "    num_return_sequences=10, \n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for i, sequence in enumerate(generated_text):\n",
    "    print(f\"Generated sequence {i+1}: {tokenizer.decode(sequence, skip_special_tokens=True)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dee5f-0dff-422d-9c55-c245aa48e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "accuracy_values = []\n",
    "\n",
    "\n",
    "for length in tqdm(range(1, 25)):\n",
    "    length_errors = []\n",
    "    length_accuracies = []\n",
    "\n",
    "\n",
    "    prompt = f\"<{length}>\"\n",
    "    outp = model.generate(\n",
    "        input_ids=tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        max_length=100,\n",
    "        temperature=0.5,\n",
    "        num_return_sequences=100,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    for generated_text in outp:\n",
    "\n",
    "        sentences = re.split('[.!?] ', tokenizer.decode(generated_text, skip_special_tokens=True))\n",
    "        first_sentence = sentences[0]\n",
    "    \n",
    "\n",
    "        num_words = len(first_sentence.split(' '))\n",
    "    \n",
    "\n",
    "        error = (num_words - length) ** 2\n",
    "        accuracy = int(num_words <= length)\n",
    "    \n",
    "\n",
    "        length_errors.append(error)\n",
    "        length_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "    mse = np.mean(length_errors)  \n",
    "    accuracy = np.mean(length_accuracies) \n",
    "\n",
    "\n",
    "    mse_values.append(mse)\n",
    "    accuracy_values.append(accuracy)\n",
    "\n",
    "print(\"MSE values:\", mse_values)\n",
    "print(\"Accuracy values:\", accuracy_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94bcc7b-2801-4c81-bcdd-584e67ac2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "\n",
    "axs[0].plot(range(1, 25), mse_values, marker='o')\n",
    "axs[0].set_title('MSE values')\n",
    "axs[0].set_xlabel('Sentence length')\n",
    "axs[0].set_ylabel('MSE')\n",
    "\n",
    "\n",
    "axs[1].plot(range(1, 25), accuracy_values, marker='o')\n",
    "axs[1].set_title('Accuracy values')\n",
    "axs[1].set_xlabel('Sentence length')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
