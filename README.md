# Исследование контроля количества слов языковых моделией

Авторы [статьи](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/KatherineLi.pdf) утверждают, что обученная с помощью добавления специальных токенов длины предложения  модель (125 М параметров) значительно превосходит такие модели, как ChatGPT и GPT-2 Large в генерации предложений заданной длины, несмотря на более малый размер. Добавление WCC токенов не испортило качество генерируемого текста. В нашей работы мы попытались воспроизвести результаты, описанные в статье, и применить данную методику к генерации абзацев заданной длины.

# Датасет

[Датасет Openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) (350k - from scratch, 10k - fine-tuning)
Разбиение на предложения и подсчет слов с помощью stanza
Разбиваем на предложения и в начале каждого предложения вставляем “токен длины” - <N+1>. Где N – длина предложения
Далее рассматриваем 3 подхода:
1. Добавление WCC токенов к предложениям без дальнейшей конкатенации
2. Добавление WCC токенов к предложением с конкатенацией
3. Добавление одного WCC токена к абзацу
Полное создание датасета заняло около 3 часов на 2-х GPU A6000
